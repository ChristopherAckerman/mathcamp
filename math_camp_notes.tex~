% Created 2020-09-11 Fri 17:15
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{url}
\usepackage{algorithm2e}
\newtheorem*{definition}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{theorem}{Theorem}
\newtheorem{question}{Question}
\usepackage[margin=1in]{geometry}
\usepackage{titlesec} % Used to customize the \section command
\usepackage{hyperref} % Required for adding links	and customizing them
\usepackage[dvipsnames]{xcolor}
\usepackage{booktabs}
\newcommand{\gr}{\textcolor{ForestGreen}}
\newcommand{\rd}{\textcolor{red}}
\newcommand{\R}{\mathbb{R}}
\author{Chris Ackerman}
\date{\today}
\title{Math Camp Notes}
\hypersetup{
 pdfauthor={Chris Ackerman},
 pdftitle={Math Camp Notes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.0.50 (Org mode 9.3)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{abstract}
These are my notes from UCLA summer math camp with Ichiro Obara. I've copied down the definitions that I thought would be useful throughout my PhD studies, usually because I learned something new, I think the definition will be used in coursework, or because it's something I was supposed to learn a long time ago but keep forgetting. If you have any questions, or if you notice any errors, please submit a  pull request or email \url{christopherackerman104@gmail.com}.
\end{abstract}

\setcounter{tocdepth}{2}
\tableofcontents

\maketitle

\newpage
\section{Single Variable Calculus I}
\label{sec:orge153958}
\begin{definition}[Graph]
The set of $(x, y)$ that $f$ passes through is called the \gr{graph} of $f$.
\end{definition}

\begin{definition}[Convergent Sequence]
A sequence ${x_n}_n$ \gr{converges to} $x^* \in \mathbb{R}$ if, for any $\varepsilon > 0$, there exists an integer $N$ such that $|x_n| \le K$ for every $n$.
\end{definition}

\begin{definition}[Continuous Function]
Function $f: X \to \mathbb{R}$ is \gr{continuous at $x \in X$} if $x_n \to x$ for $x_n \in X \implies f(x_n) \to f(x)$. $f$ is \gr{continuous} if it is continuous at every $x$ in its domain $X$. 
\end{definition}

\begin{definition}[Differentiable Function and Derivative]
A function $f$ on $(a, b)$ is \gr{differentiable} at $x \in (a, b)$ if $\frac{f(x + \Delta x) - f(x)}{\Delta x}$ converges to the same number for any sequence $\Delta x(\ne 0)$ such that $|\Delta x| \to 0$. This number is the \gr{derivative} of $f$ at $x$. $f$ is differentiable on $(a, b)$ if it is differentiable at every $x \in (a, b)$.
\end{definition}

\subsection{Rules for Taking Derivatives of Simple Analytic Functions}
\label{sec:org3f6f458}
\begin{definition}[Product Rule]
\[
\left(f(x)g(x)\right)' = f'(x)g(x) + f(x)g'(x)
\]
\end{definition}
\begin{definition}[Quotient Rule]
\[
\left(\frac{f(x)}{g(x)}\right)' = \frac{f'(x)g(x) - f(x)g'(x)}{g^2(x)}
\]
\end{definition}
\begin{definition}[Chain Rule]
Consider a composite function 
\[
h(x) = g(f(x)).
\]
If $f$ is differentiable at $x$ and $g$ is differentiable at $f(x)$, then $h$ is differentiable at $x$ and its derivative is given by
\[
h'(x) = g'(f(x)) \cdot f'(x).
\]
\end{definition}

\begin{definition}[Inverse Function]
The \gr{inverse} function $f^{-1}$ of $f$ is defined for each $x$ in the range of $f$ as the unique value that satisfies
\[
f(f^{-1}(x)) = x.
\]
If $f$ is differentiable and has an inverse $f^{-1}$, then $f^{-1}$ is differentiable and the derivative of $f^{-1}$ is the reciprocal of $f'$.
\end{definition}

\begin{definition}[Elasticity]
\gr{Elasticity} measures the percentage change of a variable with respect to a percentage change of another variable. Suppose that two variables, $x$ and $y$, satisfy $y = f(x)$. The \gr{$x$-elasticity of $y$} at $(x_0, y_0)$ is given by 
\[
\underset{\Delta x \to 0}{\lim} \frac{\frac{\Delta y}{y_0}}{\frac{\Delta x}{x_0}} = \underset{\Delta x \to 0}{\lim} \frac{\frac{f(x_0 + \Delta x) - f(x_0)}{f(x_0)}}{\frac{\Delta x}{x_0}} = f'(x_0) \frac{x_0}{f(x_0)}.
\]
We often express variables in natural logs to find the elasticity more easily. In logs, the $x$-elasticity of $y$ is
\[
\frac{d \ln y}{d \ln x}.
\]
\end{definition}
\subsection{Exercises and Solutions}
\label{sec:org98dbf3e}
\begin{question}
Let $\{x_n\}_n$ and $\{y_n\}_n$ be two convergent sequences. Show that
\begin{enumerate}
\item $\lim (x_n y_n) = \lim x_n \lim y_n$,
\item $\lim \left(\frac{x_n}{y_n}\right) = \frac{\lim x_n}{\lim y_n} $, assuming $\lim y_n \ne 0$.
\end{enumerate}
\end{question}
\begin{proof}[Answer]
$ $ \newline
\begin{enumerate}
\item
\begin{align*}
\intertext{Note that}
xy - x_n y_n &= x(y - y_n) + (x - x_n)y_n.\\
\intertext{So,}
|xy - x_n y_n| & \le |x(y - y_n)| + |(x - x_n)y_n|. \tag{Triangle Inequality}\\
\intertext{Since $\{y_n\}_n$ is a convergent (thus bounded) sequence, there exists a $K$ such that}
|(x - x_n)y_n| & \le |x - x_n| |y_n|\\
& \le |(x - x_n)|K.
\intertext{\rd{Do we need to repeat the argument for $K$ here for $\{x_n\}$?}}
\intertext{Then,}
|x(y - y_n)| & \le |x| |(y - y_n)| \to 0,\\
\intertext{and}
|(x - x_n)y_n| & \le |(x - x_n)|K \to 0.\\
\intertext{Therefore, }
|xy - x_n y_n| & \to 0.
\end{align*}
\item
\begin{align*}
\intertext{Choose $m$ so that $|y_n - y| < \frac{1}{2} y$ for $n \ge m$. Then,}
|y_n| & > \frac{1}{2}|y|. \tag{$n \ge m$}\\
\intertext{Given $\varepsilon > 0$, there is an integer $N > m$ such that $n \ge N$ implies}
|y_n - y| & < \frac{1}{2} |s|^2 \varepsilon.\\
\intertext{Hence, for $n \ge N$,}
\left|\frac{1}{y_n} - \frac{1}{y}\right| &= \left|\frac{y_n - y}{y_n y} \right|\\
& < \frac{2}{|y|^2} |y_n - y|\\
& < \varepsilon.
\end{align*}
Substitute this result into the previous proof to get the desired result.
\end{enumerate}
\end{proof}
\begin{question}
Show that a bounded increasing sequence $x_1 \le x_2 \le \ldots \le K < \infty$ must be a convergent sequence (use Bolzano-Weierstrasse).
\end{question}
\begin{proof}[Answer]
\begin{align*}
\intertext{Since $\{x_n\}_n$ is bounded between $x_1$ and $K$, it has a convergent subsequence by BWT. Let $\{x_n\}_k$ denote this subsequence, and let $x^*$ be its limit, and note that $x^* \le K$. By the definition of a limit, for any $\varepsilon > 0$, there exists a $\hat{k}$ such that}
|x_{n(k)} - x^*| & < \varepsilon \text{ for } k \ge \hat{k}.
\intertext{Because the original sequence is increasing, this also holds for any $n \ge n(k)$ in the original sequence, completing the proof.}
\end{align*}
\end{proof}
\begin{question}
Let $f:\mathbb{R} \to \mathbb{R}$ and $g: \mathbb{R} \to \mathbb{R}$ be continuous functions. Show that $h(x) \equiv g(f(x))$ is a continuous function.
\end{question}
\begin{proof}[Answer]
Start with $f(x)$ and take the limits one at a time. Use continuity at each step to show that the composite function $h$ is also continuous.
\end{proof}

\begin{question}
Derive the price elasticity of $q = -\frac{p}{3} + 8$ as a function of $p$. Note that the answer will only be defined for $p \in (0, 24)$.
\end{question}
\begin{proof}[Answer]
\begin{align*}
\intertext{Start with the definition,}
\varepsilon &= f'(p) \frac{p}{f(p)}\\
f(p) &= \frac{-p}{3} + 8\\
f'(p) &= -\frac{1}{3}\\
\varepsilon &= \frac{-1}{3} \cdot \frac{p}{\frac{-p}{3} + 8}\\
&= - \frac{1}{3} \cdot \frac{3p}{-p + 24}\\
&= \frac{-p}{24 - p}
\end{align*}
\end{proof}
\begin{question}
Derive the $x$-elasticity of $f(x) = 3x^2$ and show that it does not depend on $x$. More generally, discuss why any function $f: \mathbb{R}_+ \to \mathbb{R}_+$ with constant elasticity can be expressed as $Ax^B$ with some $A > 0$ and $B \in \mathbb{R}$.
\end{question}
\begin{proof}[Answer]
\begin{align*}
\intertext{Let's follow the same steps as the last question.}
\varepsilon = f'(x) \frac{x}{f(x)}\\
f(x) &= 3x^2\\
f'(x) &= 6x\\
\varepsilon &= \frac{6x \cdot x}{3x^2}\\
&= 2.\\
\intertext{For the second part of the question, I'm going to prove the generalized form of the previous result.}
f(x) &= Ax^B\\
f'(x) &= ABx^{B - 1}\\
\varepsilon &= \frac{AB x^{B - 1} \cdot x}{Ax^B}\\
&= \frac{ABx^B}{Ax^B}\\
&= B.\\
\intertext{\rd{This approach is different from Ichiro's solution and goes in the opposite direction. It doesn't have the same intuition from the log transform, but I think it's still sufficient.}}
\end{align*}
\end{proof}
\begin{question}
Show that the $x$-elasticity of $f(x)g(x)$ is the sum of the $x$-elasticity of $f(x)$ and the $x$-elasticity of $g(x)$.
\end{question}
\begin{proof}[Answer]
\begin{align*}
\intertext{Take the log transform of the function. More generally, this is a nice way to get separable forms of multiplicative equations.}
\frac{d \ln fg}{d \ln x} & = \frac{d \ln f + d \ln g}{d \ln x}\\
& = \frac{d \ln f}{d \ln x} + \frac{d \ln g}{d \ln x}.
\end{align*}
\end{proof}
\section{Single Variable Calculus II}
\label{sec:org5b92076}
\subsection{Linear Approximation}
\label{sec:org082e5bc}
We can use the derivative of \(f\) at \(x_0\) to construct a linear approximation of \(f\) around \(x_0\):
\[
L(x:x_0) = f(x_0) + f'(x_0)(x - x_0).
\]
This approximation is a straight line that is tanget to \(f\) at \(x_0\). How good is this approximation? We can move \(x\) by \(\Delta x\) and evaluate the ``error''.

\begin{definition}[Approximation Error]
For an approximation $L(x_0 + \Delta x: x_0)$, the \gr{approximation error $R(\Delta x; x_0)$} is 
\[
R(\Delta x; x_0) = |f(x_0 + \Delta x) - L(x_0 + \Delta x: x_0)|.
\]
\end{definition}
In the case of linear approximation,
\[
R(\Delta x; x_0) = |f(x_0 + \Delta x) - f(x_0) - f'(x_0)\Delta x|.
\]
\(\frac{R(\Delta x; x_0)}{\Delta x}\) converges to \(0\) as \(\Delta x \to 0\), so the error goes to \(0\) faster than \(\Delta x\) goes to \(0\). \(L(x: x_0)\) is the best linear approximation of \(f\) around \(x_0\).
\begin{definition}[Newton's Method]
\gr{Newton's method} is an example of an algorithm that uses linear approximations.
\begin{enumerate}
\item Guess any point $x_0$.
\item Find a solution for the linear approximation $L(x: x_0 = 0)$ instead of $f(x) = 0$, which gives $x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}$.
\item Repeat this procedure to obtain $x_1, x_2, \ldots$ by $x_{n + 1} = x_n - \frac{f(x_n)}{f'(x_n)}$.
\item If $x_n$ converges to some number $x^*$, then $x^*$ must satisfy $f(x^*) = 0$.
\end{enumerate}
\end{definition}

\begin{theorem}[Mean Value Theorem]
Suppose that $f: \mathbb{R} \to \mathbb{R}$ is differentiable and $f'$ is continuous. Pick any point $x \in \mathbb{R}$ and $\Delta x > 0$. Then there exists $t \in (x, x + \Delta x)$ such that
\[
f(x + \Delta x) - f(x) = f'(t)\Delta x.
\]
\end{theorem}
This is a baby step towards Taylor's Theorem. For intuition, suppose that \(x\) increases to \(x + \Delta x > x\) and \(f(x)\) moves to \(f(x + \Delta x)\). The rate of change is \(\frac{f(x + \Delta x) - f(x)}{\Delta x}\). For differentiable \(f\), this rate should be somewhere between the maximum derivative and the minimum derivative of \(f\) on \((x, x + \Delta x)\). The mean value theorem says that, when \(f'\) is continuous, this rate is the derivative of \(f\) at some point \(t \in (x, x + \Delta x)\).

Let's reexamine the relative speed of \(R(\Delta x; x_0) \to 0\) vs. \(\Delta x \to 0\). By the mean value theorem,
\[
R(\Delta x; x_0) = |(f'(t) - f'(x_0))\Delta x|
\]
for some \(t \in (x, x + \Delta x)\). So \(\frac{R(\Delta x; x_0)}{\Delta x}\) converges to \(0\) at the same speed \(f'(t)\) converges to \(f'(x_0)\). If \(f'\) is differentiable, then
\begin{equation*}
|f'(t) - f'(x_0)| \sim |f''(x_0)(t - x_0)| \le |f''(x_0)\Delta x|.
\end{equation*}
Intuitively, \(R(\Delta x; x_0)\) converges to \(0\) at the speed of \((\Delta x)^2\) in this case.
\subsection{Higher Order Derivatives}
\label{sec:org526a2a2}
A derivative is itself a function, so it is possible to consider the derivative of a derivative, etc.

\begin{definition}[Continuously differentiable]
A function is \gr{continuously differentiable} if its derivate is continuous. A function is \gr{$k$-times continuously differentiable} if it's $k$-times differentiable and its $k^{\text{th}}$ derivative is continuous. The set of $k$-times continuously differentiable functions is denoted by $\mathcal{C}^k$.
\end{definition}

Polynomials, exponential functions, and long functions are all \(C^\infty\). \(C^0\) denotes a continuous function.

\begin{definition}[Taylor Polynomials]
For any $f \in \mathcal{C}^n$,
\[
L^k(x: x_0) = f(x_0) + \sum^k_{i = 1} \frac{f^{(i)}}{i!}(x - x_0)^i
\] 
is the \gr{$k^{\text{th}}$-order Taylor polynomial} of $f$ around $x_0$, where $f^{(i)}$ is the $i^{\text{th}}$ derivative of $f$.
\end{definition}
Higher order polynomials are more flexible than linear functions, so they provide a better local approximation. Taylor's Theorem is a generalization of the Mean Value Theorem.
\begin{theorem}[Taylor's Theorem]
Suppose that $f: \R \to \R$ is a $\mathcal{C}^k$ function and $(k + 1)$-times differentiable. Pick any point $x \in \R$ and $\Delta x > 0$. Then there exists $t \in (x, x + \Delta x)$ such that 
\[
f(x + \Delta x) = L^k(x + \Delta x: x) + \frac{f^{(k + 1)}(t)}{(k + 1)!}(\Delta x)^{k + 1}.
\]
\end{theorem}

If \(f\) is a \(C^2\) function, then
\[
R(\Delta x; x_0) = \left|\frac{f''(t)}{2}(\Delta x)^2\right|,
\]
so \(R(\Delta x; x_0)\) converges to \(0\) at the speed of \((\Delta x)^2\). More generally, if \(f\) is a \(C^{k + 1}\) function, then the difference between \(f(x_0 + \Delta x)\) and the \(k^{\text{th}}\) order Taylor polynomial
\[
R_k(\Delta x; x_0) = |f(x_0 + \Delta x) - L^k(\Delta x; x_0)|
\]
converges to \(0\) at the speed of \((\Delta x)^{k + 1}\).

This formula has an important implication for local behavior. Whether \(f\) is stricly increasing or decreasing \emph{locally} around \(x_0\) is determined by the sign of \(f'(x_0)\). If \(f'(x_0) = 0\), then local behavior is determined by the sign of \(f''(x_0)\), etc. Intuitively, the local effect of each derivative dominates the local effect of all higher order derivatives.
\subsection{Exercises and Solutions}
\label{sec:org9c724c1}
\begin{question}
Let $f$ be a $\mathcal{C}^1$ function and suppose that $f'(x) > 0$ at $x$. Show that there exists $\varepsilon > 0$ such that $f$ is strictly increasing in $(x - \varepsilon, x + \varepsilon)$. Is the converse true?
\end{question}

\begin{proof}[Answer]
\begin{align*}
\intertext{This is a fairly straightforward application of Taylor's theorem where $\Delta x = \pm \varepsilon.$}
f(x - \varepsilon) &= f(x) + \underbrace{f'(t)(-\varepsilon)}_{(-)}\\
f(x - \varepsilon) &= f(x) + \underbrace{f'(t)(+\varepsilon)}_{(+)}\\
\intertext{Combining these inequalities satisfies the definition of an increasing function,}
f(x - \varepsilon) < f(x) &< f(x + \varepsilon).
\end{align*}
\end{proof}
\begin{question}
\gr{L'H\^opital's Rule.} Suppose $f$ and $g$ are differentiable at $f'$, $f(x') = g(x') = 0$, and $g'(x') \ne 0$. Show that $\lim_{x \to x'} \frac{f(x)}{g(x)} = \frac{f'(x')}{g'(x')}$.
\end{question}
\begin{proof}[Answer]
I'm going to use slightly different notation because I find $x'$ cumbersome. This is a straightforward application of the definition of a first-order Taylor expansion.

\begin{align*}
f(x) &= f(x) + f'(x) \Delta x \\
g(x) &= g(x) + g'(x) \Delta x \\
\lim_{\Delta x \to 0} \frac{f(x)}{g(x)} &= \frac{\overbrace{f(x)}^0 + f'(x) \Delta x}{\underbrace{g(x)}_0 + g'(x) \Delta x}\\
&= \frac{f'(x) \Delta x}{g'(x) \Delta x}\\
&= \frac{f'(x)}{g'(x)}
\end{align*}
\end{proof}
\begin{question}
For a $\mathcal{C}^2$ function, verify that
\[
L^2(x_0 + \Delta x: x_0) = f(x_0) + f'(x_0) \Delta x + \frac{f''(x_0)}{2}(\Delta x)^2
\]
 is the only $2^{\text{nd}}$ order polynomial that satisfies
\[
\frac{|f(x_0 + \Delta x) - L^2(x_0 + \Delta x: x_0)|}{(\Delta x)^2} \to 0
\]
as $x \to 0$.
\end{question}
\begin{proof}[Answer]
This solution is a proof by contradiction, in three parts. Since the denominator is going to zero, any value in the numerator that doesn't go to zero faster will blow up, and ruin convergence. Let $A + B\Delta x + C \Delta x^2$ be an arbitrary second order polynomial in $\Delta x$, and consider
\[
\frac{|f(x + \Delta x) - \left(A + B \Delta x + C (\Delta x)^2\right)|}{(\Delta x)^2}.
\]
\begin{enumerate}
\item First consider $A \ne f(x_0)$. If $A \ne f(x_0)$, then the numerator converges to $|f(x_0) - A| \ne 0$.
\item Now consider $B \ne f'(x_0)$. If $B \ne f'(x_0)$, then the numerator divided by $\Delta x$ converges to $f'(x_0) - B \ne 0$.
\item Finally, consider $C \ne \frac{f''(x_0)}{2}$. In this case, the limit converges to $\left|\frac{f''(x_0)}{2} - C\right|$, which is only $0$ if $C = \frac{f''(x_0)}{2}$.
\end{enumerate}
Because of the rate at which the denominator goes to zero, the first two cases blow up, and the third case converges to the wrong number.
\end{proof}
\section{Introduction to Optimization}
\label{sec:orge3924f3}
Throughout this section we assume that our functions are continuous (and their derivatives, etc. are continuous where necessary).
\begin{definition}[Simple Optimization Problem]
\[
\max_{x \in [a, b]} f(x)
\]
or
\[
\max_{x \in \R} f(x) \text{ s.t. } x \ge a \land x \le b
\]
\end{definition}

\begin{example}[Monopoly Profit Maximization]
Consider a monopoly firm with price setting power. If it sets price $p \ge 0$ for its product, it can sell $D(p)$ units. Each unit costs $c$ to produce and there is a fixed cost $F$ to start production. This firm's profit maximization problem (assuming they can shut down/choose not to produce) is 
\[
\max \left\{\max_{p \ge 0}(p - c)D(p) - F, 0\right\}.
\]
The profit maximizing price $p^*$ satisfies the first order condition
\[
D(p) + (p - c)D'(p) = 0.
\]
Therefore the negative price elasticity of demand is the reciprocal of the profit margin at the optimal price $p^*$:
\[
-\mathcal{E}_D(p^*) = \frac{p^*}{p^* - c}.
\]
\end{example}
\begin{theorem}[First Order Condition]
If $f$ achieves its maximum (or minimum) value at $x^*$ on $(a, b)$, then $f'(x^*) = 0.$
\end{theorem}
\subsection{Constrained Optimization}
\label{sec:orgb7eebcf}
When we optimize over a closed and bounded interval, we're guaranteed to get a maximum and a minimum point.
\begin{theorem}[Extreme Value Theorem]
If $f: \R \to \R$ is continuous, then it has a maximum point and a minimum point over any closed and bounded interval $[a, b]$.
\end{theorem}
\begin{definition}[Necessary conditions for a maximum]
Suppose that $x^*$ maximizes $f$ on $[a, b]$. There are three possible cases, and each yields a set of necessary conditions that $x^*$ must satisfy.
\begin{enumerate}
\item $x^* \in (a, b) \implies f'(x^*) = 0$
\item $x^* = a \implies f'(a) \le 0$
\item $x^* = b \implies f'(b) \ge 0$
\end{enumerate}
\end{definition}
\begin{definition}[Penalty/Shadow Price]
Consider the cast with $x^* = b$. We can express the optimality condition with an equation
\[
f'(x) - \lambda = 0\text{, 
where \lambda = f'(b) \ge 0.}
\]
This has two useful economic interpretations.
\begin{enumerate}
\item $\lambda$ is an artifical "penalty" or marginal cost that is associated with going beyond the constraint $x \le b$. This interpretation allows us to turn the original constrained problem into an unconstrained problem with a penalty.
\item $\lambda$ is the value of relaxing the constraint $x \le b$. If $b$ increases by $\Delta b > 0$, then the maximized value of $f$ will increase by roughly $\lambda \Delta b$.
\end{enumerate}
\end{definition}
The first order condition is necessary for a maximum/minimum, but not sufficient. For continuous functions, though, we can look at the (sufficient) second order condition
\begin{definition}[Second Order Condition]
Suppose that $f$ is $\mathcal{C}^2$ and $f'(x^*) = 0$ at $x^* \in (a, b)$.
\begin{enumerate}
\item If $f''(x^*) < 0$, then $x^*$ is a strict local maximum for $f$.
\item If $f''(x^*) > 0$, then $x^*$ is a strict local minimum for $f$.
\end{enumerate}
\end{definition}
\begin{definition}[Concavity and Convexity]
$ $\\
\begin{enumerate}
\item $f$ is \gr{concave} if
\[
f((1 - \lambda)x + \lambda y) \ge (1 - \lambda)f(x) + \lambda f(y)
\]
for every $x, y \in X$ and $\lambda \in [0, 1]$.
\item $f$ is \gr{convex} if
\[
f((1 - \lambda)x + \lambda y) \le (1 - \lambda)f(x) + \lambda f(y)
\]
for every $x, y \in X$ and $\lambda \in [0, 1]$.
\end{enumerate}
For \gr{strictly concave} and \gr{strictly convex}, make the inequalities strict.
\end{definition}
\begin{definition}[Useful facts about concave functions]
$$\\
\begin{enumerate}
\item If $f$ is differentiable,
\[
f \text{ is concave } \iff f(x) \le f(x_0) + f'(x_0)(x - x_0)\ \forall x \in X \text{ given any } x_0.
\]
\item If $f$ is twice differentiable,
\[
f \text{ is concave } \iff f''(x) \le 0 \ \forall x \in X.
\]
\end{enumerate}
Similar results hold for convex functions; note that $f$ is concave $\iff -f$ is convex.
\end{definition}
\begin{theorem}[FOC Sufficiency]
Suppose that $f$ is a concave function on $(a, b)$ and $f'(x^*) = 0$ at $x^* \in (a, b)$. Then $f$ achieves its maximum value on $(a, b)$ at $x^*$.
\end{theorem}
\begin{proof}
Since $f$ is concave, 
\[
f(x) \le f(x^*) + f'(x^*)(x - x^*) = f(x^*)
\]
for every $x \in (a, b)$.
\end{proof}
\begin{theorem}[Another Sufficiency Condition]
Suppose that $x^* \in (a, b)$ is the only point in $(a, b)$ that satisfies the FOC $f'(x^*) = 0$. If $f''(x^*) < 0$, then $x^*$ must be a maximum for $f$ on $(a, b)$.
\end{theorem}
\rd{Does the strict inequality here mean that this is the unique maximum point for $f$ on $(a, b)$?}
\subsection{Exercises and Solutions}
\label{sec:orgb465d54}
\begin{question}
Consider
\[
\max_{x \in (a, b]}f(x).
\]
What necessary condition must the optimal solution $x^*$ satisfy?
\end{question}
\begin{proof}[Answer]
$ $\\
\begin{enumerate}
\item If $x^* \ne b, f'(x^*) = 0$.
\item If $x^* = b, f'(b) \ge 0$.
\end{enumerate}
\end{proof}
\begin{question}
Solve the following optimization problems.
\begin{enumerate}
\item 
\[
\max_{x \in \R++} f(x) = \ln x - 3x
\]
\item 
\[
\max_{x \in \R+} -x^3 + 4x^2 - 5x -2
\]
\end{enumerate}
\end{question}
\begin{proof}[Answer]
\begin{enumerate}
\item
\begin{align*}
f' &= \frac{1}{x} - 3\\
\frac{1}{x} - 3 &= 0\\
x &= \frac{1}{3}
\end{align*}
\end{enumerate}
\end{proof}
\begin{question}
A consumer spends $x$\$ out of $100$\$ to buy some product at price $p > 0$. Suppose that her utility when purchasing $x \in \left[0, \frac{100}{p}\right] units of the product is given by
\[
u(x) = \frac{2x}{x + 1} + 100 - px.
\]
Find her utility-maximizing consumption $x(p)$ as a function of price.
\end{question}
\begin{proof}[Answer]
\begin{align*}
\intertext{We know that she will spend her entire budget on the good if purchasing a single unit provides positive utility (local nonsatiation), or that she will not purchase any units if the price is too high (rationality). To figure out how much the consumer buys if she does purchase some of the good, take the derivative of demand and solve the FOC.}
\frac{\partial f}{\partial x} &= \frac{2 - p(x - 1)^2}{(x + 1)^2}\\
0 &= \frac{2 - p(x - 1)^2}{(x + 1)^2}\\
\intertext{Since prices must be positive, take only the positive root. And because the consumer can choose not to buy anything, also include a maximization problem with the outside option in her demand function.}
x(p) &= \max \left\{\frac{\sqrt{p^2 + 196p + 10404} - p + 102}{2p}, 0\right\}
\end{align*}
\end{proof}
\end{document}